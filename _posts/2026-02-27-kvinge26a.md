---
title: Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the
  Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations
year: '2025'
booktitle: Proceedings of the 1st Conference on Topology, Algebra, and Geometry in
  Data Science(TAG-DS 2025)
openreview: 74Xpma3Lha
abstract: While a real-world research program in mathematics may be guided by a motivating
  question, the process of mathematical discovery is typically open-ended. Ideally,
  exploration needed to answer the original question will reveal new structures, patterns,
  and insights that are valuable in their own right. This contrasts with the exam-style
  paradigm in which the machine learning community typically applies AI to math. To
  maximize progress in mathematics using AI, we will need to go beyond simple question
  answering. With this in mind, we explore the extent to which narrow models trained
  to solve a fixed mathematical task learn broader mathematical structure that can
  be extracted by a researcher or other AI system. As a basic test case for this,
  we use the task of training a neural network to predict a group operation (for example,
  performing modular arithmetic or composition of permutations). We describe a suite
  of tests designed to assess whether the model captures significant group-theoretic
  notions such as the identity element, commutativity, or subgroups. Through extensive
  experimentation we find evidence that models learn representations capable of capturing
  abstract algebraic properties. For example, we find hints that models capture the
  commutativity of modular arithmetic. We are also able to train linear classifiers
  that reliably distinguish between elements of certain subgroups (even though no
  labels for these subgroups are included in the data). On the other hand, we are
  unable to extract notions such as the concept of the identity element. Together,
  our results suggest that in some cases the representations of even small neural
  networks can be used to distill interesting abstract structure from new mathematical
  objects.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kvinge26a
month: 0
tex_title: Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into
  the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group
  Operations
firstpage: 302
lastpage: 312
page: 302-312
order: 302
cycles: false
bibtex_author: Kvinge, Henry and Aguilar, Andrew and Farnsworth, Nayda and O'Brien,
  Grace and Jasper, Robert and Scullen, Sarah and Jenne, Helen
author:
- given: Henry
  family: Kvinge
- given: Andrew
  family: Aguilar
- given: Nayda
  family: Farnsworth
- given: Grace
  family: Oâ€™Brien
- given: Robert
  family: Jasper
- given: Sarah
  family: Scullen
- given: Helen
  family: Jenne
date: 2026-02-27
address:
container-title: Proceedings of the 1st Conference on Topology, Algebra, and Geometry
  in Data Science(TAG-DS 2025)
volume: '321'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 2
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v321/main/assets/kvinge26a/kvinge26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
