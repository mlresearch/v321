---
title: Learning Polynomial Activation Functions for Deep Neural Networks
year: '2025'
booktitle: Proceedings of the 1st Conference on Topology, Algebra, and Geometry in
  Data Science(TAG-DS 2025)
openreview: E7InqVkZkv
abstract: Activation functions are crucial for deep neural networks. This novel work
  frames the problem of training neural network with learnable polynomial activation
  functions as a polynomial optimization problem, which is solvable by the Moment-SOS
  hierarchy. This work represents a fundamental departure from the conventional paradigm
  of training deep neural networks, which relies on local optimization methods like
  backpropagation and gradient descent. Numerical experiments are presented to demonstrate
  the accuracy and robustness of optimum parameter recovery in presence of noises.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang26a
month: 0
tex_title: Learning Polynomial Activation Functions for Deep Neural Networks
firstpage: 90
lastpage: 99
page: 90-99
order: 90
cycles: false
bibtex_author: Zhang, Linghao and Nie, Jiawang and Tang, Tingting
author:
- given: Linghao
  family: Zhang
- given: Jiawang
  family: Nie
- given: Tingting
  family: Tang
date: 2026-02-27
address:
container-title: Proceedings of the 1st Conference on Topology, Algebra, and Geometry
  in Data Science(TAG-DS 2025)
volume: '321'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 2
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v321/main/assets/zhang26a/zhang26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
